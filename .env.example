# ==============================================================================
# Azure OpenAI Configuration
# ==============================================================================
# Your Azure OpenAI API key (from Azure Portal > Cognitive Services > Keys)
AZURE_OPENAI_API_KEY=your_api_key_here

# Azure OpenAI endpoint URL (e.g., https://swedencentral.api.cognitive.microsoft.com)
AZURE_OPENAI_ENDPOINT=https://swedencentral.api.cognitive.microsoft.com

# API version for Azure OpenAI
AZURE_OPENAI_API_VERSION=2024-08-01-preview

# Deployment name for chat model (e.g., gpt-4o-mini)
AZURE_OPENAI_CHAT_DEPLOYMENT=chat-model

# Deployment name for embedding model (e.g., text-embedding-3-large)
AZURE_OPENAI_EMBEDDING_DEPLOYMENT=embedding-model

# ==============================================================================
# Vector Store Configuration
# ==============================================================================
# Directory to persist Chroma vector store
VECTORSTORE_DIR=./vectorstore

# Collection name in vector store
VECTORSTORE_COLLECTION=support_docs

# ==============================================================================
# Chunking Configuration
# ==============================================================================
# Maximum tokens per chunk (recommended: 800-1200)
CHUNK_SIZE_TOKENS=1000

# Overlap tokens between chunks (recommended: 150-300)
CHUNK_OVERLAP_TOKENS=200

# ==============================================================================
# Retrieval Configuration
# ==============================================================================
# Number of chunks to retrieve for each query
RETRIEVAL_TOP_K=5

# Maximum tokens to send as context to LLM
CONTEXT_TOKEN_BUDGET=3000

# ==============================================================================
# LLM Configuration
# ==============================================================================
# Temperature for LLM responses (0.0 = deterministic, 1.0 = creative)
LLM_TEMPERATURE=0.1

# Maximum tokens for LLM response
LLM_MAX_TOKENS=512

# ==============================================================================
# Logging Configuration
# ==============================================================================
# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# Log file path (leave empty for console only)
LOG_FILE=logs/app.log

# ==============================================================================
# Application Configuration
# ==============================================================================
# Application environment (development, staging, production)
APP_ENV=development

# Enable/disable caching of embeddings
ENABLE_EMBEDDING_CACHE=true
